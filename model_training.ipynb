{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\artha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\__init__.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     58\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\artha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\compat\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     IS64,\n\u001b[0;32m     19\u001b[0m     ISMUSL,\n\u001b[0;32m     20\u001b[0m     PY310,\n\u001b[0;32m     21\u001b[0m     PY311,\n\u001b[0;32m     22\u001b[0m     PY312,\n\u001b[0;32m     23\u001b[0m     PYPY,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ISMUSL' from 'pandas.compat._constants' (c:\\Users\\artha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\compat\\_constants.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\artha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython setup.py build_ext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_err\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     get_option,\n\u001b[0;32m     70\u001b[0m     set_option,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     options,\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from nltk.corpus import wordnet  # For accessing synonyms\n",
    "import nlpaug.augmenter.word as naw  # Library for text augmentation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 24.0kB/s]\n",
      "c:\\Users\\artha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\artha\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.95MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 8.05MB/s]\n",
      "Downloading config.json: 100%|██████████| 483/483 [00:00<00:00, 241kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:13<00:00, 19.8MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library is working correctly!\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "print(\"Transformers library is working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m splits \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit/train-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit/validation-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit/test-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://datasets/dair-ai/emotion/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m splits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "splits = {'train': 'split/train-00000-of-00001.parquet', 'validation': 'split/validation-00000-of-00001.parquet', 'test': 'split/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/dair-ai/emotion/\" + splits[\"train\"])\n",
    "print(f\"Number of rows in the dataset: {len(df)}\")\n",
    "print(df.head())\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "# Removing punctuation\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "# Removing special characters\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "# Handle missing values\n",
    "df.dropna(subset=['text', 'label'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar chart of label distribution\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(label_counts.index, label_counts.values)\n",
    "plt.title('Distribution of Emotion Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sayantan Datta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Sayantan Datta/nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sayantan Datta\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nlpaug\\model\\word_dict\\wordnet.py:58\u001b[0m, in \u001b[0;36mWordNet.pos_tag\u001b[1;34m(cls, tokens)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Sayantan Datta/nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sayantan Datta\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Augmented samples\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     aug_df \u001b[38;5;241m=\u001b[39m class_df\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39mn_aug, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m     aug_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43maug_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugment_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     balanced_dfs\u001b[38;5;241m.\u001b[39mappend(aug_df)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36maugment_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mSynonymAug(aug_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     augmented_text \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m augmented_text\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nlpaug\\base_augmenter.py:119\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, data, n, num_thread)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [action_fx(clean_data) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nlpaug\\base_augmenter.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [\u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nlpaug\\augmenter\\word\\synonym.py:121\u001b[0m, in \u001b[0;36mSynonymAug.substitute\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    117\u001b[0m doc \u001b[38;5;241m=\u001b[39m Doc(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(data))\n\u001b[0;32m    119\u001b[0m original_tokens \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mget_original_tokens()\n\u001b[1;32m--> 121\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m aug_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_aug_idxes(pos)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_idxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(aug_idxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nlpaug\\model\\word_dict\\wordnet.py:61\u001b[0m, in \u001b[0;36mWordNet.pos_tag\u001b[1;34m(cls, tokens)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Sayantan Datta/nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sayantan Datta\\\\anaconda3\\\\envs\\\\pytorch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sayantan Datta\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def augment_text(text):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_text = aug.augment(text)[0]\n",
    "    return augmented_text\n",
    "\n",
    "# Create balanced dataset with augmentation\n",
    "balanced_dfs = []\n",
    "max_size = df['label'].value_counts().max()\n",
    "\n",
    "for label in df['label'].unique():\n",
    "    class_df = df[df['label'] == label]\n",
    "    if len(class_df) < max_size:\n",
    "        # Calculate how many augmented samples we need\n",
    "        n_aug = max_size - len(class_df)\n",
    "        # Original samples\n",
    "        balanced_dfs.append(class_df)\n",
    "        \n",
    "        # Augmented samples\n",
    "        aug_df = class_df.sample(n=n_aug, replace=True)\n",
    "        aug_df['text'] = aug_df['text'].apply(augment_text)\n",
    "        balanced_dfs.append(aug_df)\n",
    "    else:\n",
    "        balanced_dfs.append(class_df)\n",
    "\n",
    "balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "dataset = Dataset.from_pandas(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 32172\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sayantan Datta\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map:  99%|█████████▉| 32000/32172 [00:15<00:00, 2038.83 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Set up training arguments\u001b[39;00m\n\u001b[0;32m     12\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     13\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3454\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3455\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3456\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3458\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3462\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3466\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3324\u001b[0m     }\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtokenize_function\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Sayantan Datta\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3081\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3078\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 3081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3082\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3084\u001b[0m     )\n\u001b[0;32m   3086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   3087\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3088\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3090\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=31)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
